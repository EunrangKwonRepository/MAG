{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pandas import Series\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Extracting papers between 2016 and 2020 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### extracting bibliography data between 2016 and 2020 in Papers.txt which is given by MAG ###\n",
    "## extracting online/offline data by offline date and online date\n",
    "## index number 8(offline date) and 9(online date)\n",
    "## So It is possible to include duplicated papers because it is divided by online and offline date\n",
    "\n",
    "filename = \"Papers.txt\" #MAG data\n",
    "chunksize = 10000\n",
    "\n",
    "for cnt, chunk in enumerate(pd.read_table(filename, chunksize=chunksize, header=None, quoting = 3)):\n",
    "    i = 0\n",
    "    number = 0\n",
    "    try:\n",
    "        while i == number:\n",
    "            cnt == number\n",
    "            chunk[8]= pd.to_datetime(chunk[8], format='%Y-%m-%d') #chunk[8] -> Date, chunk[9] -> OnlineDate\n",
    "            df = chunk.loc[chunk[8]> '2015', :]\n",
    "            df.to_csv('papers_2016.txt', index=False, header=None, sep=\"\\t\", mode='a')\n",
    "            i +=1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for cnt, chunk in enumerate(pd.read_table(filename, chunksize=chunksize, header=None, quoting = 3)):\n",
    "    i = 0\n",
    "    number = 0\n",
    "    try:\n",
    "        while i == number:\n",
    "            cnt == number\n",
    "            chunk[9]= pd.to_datetime(chunk[9], format='%Y-%m-%d') #chunk[8] -> Date, chunk[9] -> OnlineDate\n",
    "            df = chunk.loc[chunk[9]> '2015', :]\n",
    "            df.to_csv('papers_2016_online.txt', index=False, header=None, sep=\"\\t\", mode='a')\n",
    "            i +=1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "filename = \"papers_2016.txt\"\n",
    "chunksize = 10000\n",
    "\n",
    "for cnt, chunk in enumerate(pd.read_table(filename, chunksize=chunksize, header=None, quoting = 3)):\n",
    "    i = 0\n",
    "    number = 0\n",
    "    while i == number:\n",
    "        cnt == number\n",
    "        chunk = chunk[[0, 3, 7, 8, 9, 11, 12]]\n",
    "        chunk[13] = 1\n",
    "        chunk.to_csv('papers_2016_sub1.txt', index=False, header=None, sep=\"\\t\", mode='a')\n",
    "        i +=1\n",
    "        \n",
    "filename = \"papers_2016_online.txt\"\n",
    "chunksize = 10000\n",
    "\n",
    "for cnt, chunk in enumerate(pd.read_table(filename, chunksize=chunksize, header=None, quoting = 3)):\n",
    "    i = 0\n",
    "    number = 0\n",
    "    while i == number:\n",
    "        cnt == number\n",
    "        chunk = chunk[[0, 3, 7, 8, 9, 11, 12]]\n",
    "        chunk[13] = 2\n",
    "        chunk.to_csv('papers_2016_sub2.txt', index=False, header=None, sep=\"\\t\", mode='a')\n",
    "        i +=1\n",
    "        \n",
    "df1 = pd.read_table(\"papers_2016_sub1.txt\", header=None)\n",
    "df2 = pd.read_table(\"papers_2016_sub2.txt\", header=None)\n",
    "df3 = pd.concat([df1, df2]) \n",
    "df3 = df3.drop_duplicates()\n",
    "\n",
    "df3 = df3.loc[df3[1].notnull(), :] # paper_type is not null\n",
    "#papers_2016_sub_total: paperid, paper_type, year, date, onlinedate, journalid, conferid\n",
    "df3.to_csv(\"papers_2016_sub_total.txt\", index=False, header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Spliting date by year and month ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Pretreatment date ###\n",
    "## Data has two date(offline and online)\n",
    "## Formation of date is \"2020-01-01\", so let's split date by using \"-\"\n",
    "\n",
    "df = pd.read_table(\"papers_2016_sub_total.txt\", header=None)\n",
    "\n",
    "year1 = []; year2 = []\n",
    "\n",
    "for i in df[3]:\n",
    "    i = str(i)\n",
    "    j = i.split(\"-\")[0]\n",
    "    j = re.sub('[^0-9]', '', j) #some data dosent'have date format\n",
    "    year1.append(j)\n",
    "\n",
    "for i in df[4]:\n",
    "    i = str(i)\n",
    "    j = i.split(\"-\")[0]\n",
    "    j = re.sub('[^0-9]', '', j) \n",
    "    year2.append(j)\n",
    "\n",
    "df[\"year1\"] = year1    \n",
    "df[\"year2\"] = year2\n",
    "\n",
    "df.loc[df['year1']=='', 'year1'] = 0\n",
    "df.loc[df['year2']=='', 'year2'] = 0\n",
    "\n",
    "df['year1'] = df['year1'].astype(int)\n",
    "df['year2'] = df['year2'].astype(int)\n",
    "\n",
    "over1 = df.loc[(df['year1']>=2016)&(df['year1']<=2020), :]\n",
    "over1 = over1.loc[(over1['year2']==0)|(over1['year2']>=2016)&(over1['year2']<=2020), :]\n",
    "\n",
    "over2 = df.loc[(df['year1']>=2016)&(df['year2']>=2016)&(df['year2']<=2020), :]\n",
    "over2 = over2.loc[(over2['year1']==0)|(over2['year1']>=2016)&(over2['year1']<=2020), :]\n",
    "\n",
    "off = over1.loc[over1[7]==1, :] #date -> 1, onlinedate ->2\n",
    "on = over2.loc[over2[7]==2, :]\n",
    "\n",
    "df2 = pd.concat([off, on])\n",
    "\n",
    "month1 = []; month2 = []\n",
    "for i in df2[3]:\n",
    "    i = str(i)\n",
    "    if \"-\" in i:\n",
    "        j = i.split(\"-\")[1]\n",
    "        j = re.sub('[^0-9]', '', j)\n",
    "    else:\n",
    "        j = 0\n",
    "    month1.append(j)\n",
    "    \n",
    "for i in df2[4]:\n",
    "    i = str(i)\n",
    "    if \"-\" in i:        \n",
    "        j = i.split(\"-\")[1]\n",
    "        j = re.sub('[^0-9]', '', j)\n",
    "    else:\n",
    "        j = 0\n",
    "    month2.append(j)\n",
    "    \n",
    "df2['month1'] = month1\n",
    "df2['month2'] = month2\n",
    "\n",
    "df2['month1'] = df2['month1'].astype(int)\n",
    "df2['month2'] = df2['month2'].astype(int)\n",
    "\n",
    "df2 = df2[[0, 1, 5, 6, 7, 'year1', 'year2', 'month1', 'month2']]\n",
    "df2.columns = ['paperid', 'paper_type', 'jourid', 'confid', 'onoff', 'year1', 'year2', 'month1', 'month2']\n",
    "\n",
    "df2.loc[(df2['month1']>=1)&(df2['month1']<=3), 'quarter1']=1\n",
    "df2.loc[(df2['month1']>=4)&(df2['month1']<=6), 'quarter1']=2\n",
    "df2.loc[(df2['month1']>=7)&(df2['month1']<=9), 'quarter1']=3\n",
    "df2.loc[(df2['month1']>=10)&(df2['month1']<=12), 'quarter1']=4\n",
    "\n",
    "df2.loc[(df2['month2']>=1)&(df2['month2']<=3), 'quarter2']=1\n",
    "df2.loc[(df2['month2']>=4)&(df2['month2']<=6), 'quarter2']=2\n",
    "df2.loc[(df2['month2']>=7)&(df2['month2']<=9), 'quarter2']=3\n",
    "df2.loc[(df2['month2']>=10)&(df2['month2']<=12), 'quarter2']=4\n",
    "\n",
    "#paeprs_2016_sub_total2:'paperid', 'paper_type', 'jourid', 'confid', 'onoff', 'year1', 'year2', 'month1', 'month2', 'quarter1', 'quarter2'\n",
    "df2.to_csv('papers_2016_sub_total2.txt', index=False, sep=\"\\t\")\n",
    "\n",
    "# we use data between 2016 and 2020\n",
    "df2[\"paperid\"].to_csv('paperid.txt', header=None, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Classifying gender and number of authors ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### extract authorid and author sequence number between 2016 and 2020 ###\n",
    "\n",
    "filename = \"PaperAuthorAffiliations.txt\" #MAG data\n",
    "chunksize = 10000\n",
    "\n",
    "for cnt, chunk in enumerate(pd.read_table(filename, chunksize=chunksize, header=None, quoting = 3)):\n",
    "    i = 0\n",
    "    number = 0\n",
    "    while i == number:\n",
    "        cnt == number\n",
    "        df =chunk[[0, 1, 3]] #paperid, authorid, authorsequencenumber\n",
    "        df.to_csv('paperauthor.txt', index=False, header=None, sep=\"\\t\", mode='a')\n",
    "        i +=1\n",
    "        \n",
    "paper_id = pd.read_table('paperid.txt', header=None) #paperid between 2016 and 2020\n",
    "paper_id = paper_id.drop_duplicates()\n",
    "paper_id[\"over16\"] = 1\n",
    "paper_id.rename(columns = {0: 'id'}, inplace = True)\n",
    "\n",
    "#filtering paperid in paperauthor between 2016 and 2020\n",
    "df = pd.read_table(\"paperauthor\", header=None, encoding = 'ISO-8859-1', quoting = 3) \n",
    "df.rename(columns = {0: 'id'}, inplace = True)\n",
    "\n",
    "df = pd.merge(df, paper_id, left_on='id', right_on='id', how='left')\n",
    "df = df.loc[df['over16']==1, :]\n",
    "\n",
    "#paper_author: paperid, authorid, authorsequencenumber, over16\n",
    "df.to_csv(\"paper_author.txt\", index=False, header=None, sep=\"\\t\", mode='a') \n",
    "\n",
    "#extracting authorid between 2016 and 2020\n",
    "filename = \"paper_author.txt\"\n",
    "\n",
    "for cnt, chunk in enumerate(pd.read_table(filename, chunksize=chunksize, header=None, quoting = 3)):\n",
    "    i = 0\n",
    "    number = 0\n",
    "    while i == number:\n",
    "        cnt == number\n",
    "        df =chunk[[1]]\n",
    "        df.to_csv('authorid.txt', index=False, header=None, sep=\"\\t\", mode='a')\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### classifying gender by using author's name ###\n",
    "## extracting authorid paperid authorname gender\n",
    "\n",
    "author = pd.read_table('authorid.txt', header=None, error_bad_lines=False) #load authorid between 2016 and 2020\n",
    "author = author.drop_duplicates()\n",
    "author.rename(columns = {0: 'id'}, inplace = True)\n",
    "author[\"over16\"] = 1\n",
    "\n",
    "filename = \"Authors.txt\" #MAG data\n",
    "\n",
    "for cnt, chunk in enumerate(pd.read_table(filename, chunksize=chunksize, header=None, quoting = 3)):\n",
    "    i = 0\n",
    "    number = 0\n",
    "    while i == number:\n",
    "        cnt == number\n",
    "        df =chunk[[0, 2, 4]] #authorid, nomalized name, lastaffiliationid\n",
    "        df.to_csv('authors_sub.txt', index=False, header=None, sep=\"\\t\", mode='a')\n",
    "        i +=1\n",
    "        \n",
    "df = pd.read_table('authors_sub', header=None, encoding = 'ISO-8859-1', quoting = 3) #, \n",
    "df.rename(columns = {0: 'id'}, inplace = True)\n",
    "\n",
    "df = pd.merge(df, author, left_on='id', right_on='id', how='left')\n",
    "df = df.loc[df['over16']==1, :]\n",
    "\n",
    "#author_name: authorid, nomalized name, lastaffiliationid, over16\n",
    "df.to_csv(\"author_name.txt\", index=False, header=None, sep=\"\\t\")\n",
    "\n",
    "#We got gender information of \"author_name.txt\"'s nomalized name by using genderize api\n",
    "#We then infer gender from the authors' first name, which is the first token of the full name split by space using genderize\n",
    "gender = pd.read_table('gender_infor.txt') #Gnderize data(https://genderize.io/)\n",
    "\n",
    "author_name = pd.read_table('author_name.txt', header=None)\n",
    "author_name['name'] = author_name.apply(lambda x : x[1].split(\" \")[0], axis = 1)\n",
    "author_name = pd.merge(author_name, gender, left_on= 'name', right_on='name', how='left')\n",
    "\n",
    "author_name.rename(columns = {0: 'authorid'}, inplace = True)\n",
    "author_name = author_name[['authorid', 'gender']]\n",
    "author_name.loc[author_name['gender']=='None', 'gender'] = np.nan \n",
    "author_name = author_name.loc[author_name['gender'].notnull(), :]\n",
    "\n",
    "female = {'female':1, 'male':0}\n",
    "author_name = author_name.replace(female)\n",
    "\n",
    "#author_gender: authorid, gender(female=1, male=0)\n",
    "author_name.to_csv('author_gender.txt', sep=\"\\t\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### making variable(female_have, female_first, female_last) and number of authors in each paper\n",
    "#female_have: including any female author in paper=1, male=0, unknown=np.nan \n",
    "#female_first first author is female=1, male=0, unknown=np.nan\n",
    "#female_last: last author is female=1, male=0, 알수없음=np.nan\n",
    "\n",
    "author_gender = pd.read_table('author_gender.txt')\n",
    "author_gender.columns = ['authorid', 'gender']\n",
    "\n",
    "df = pd.read_table(\"paper_author.txt\", header=None)\n",
    "df = df.drop([3], axis=1)\n",
    "df.columns = ['paperid', 'authorid', 'sequence_number']\n",
    "df = pd.merge(df, author_gender, on='authorid', how='left')\n",
    "\n",
    "## female_have\n",
    "female1 = df[['paperid', 'gender']]\n",
    "female1 = female1.drop_duplicates()\n",
    "\n",
    "lst1 = list(female1.loc[female1['gender']==1, :]['paperid']) #female\n",
    "lst2 = list(female1.loc[female1['gender']==0, :]['paperid'])\n",
    "lst3 = list(set(lst2) - set(lst1)) #male\n",
    "\n",
    "female = pd.DataFrame(lst1)\n",
    "female['female_have'] =1\n",
    "\n",
    "male = pd.DataFrame(lst3)\n",
    "male['female_have'] = 0\n",
    "\n",
    "lst = pd.concat([female, male])\n",
    "\n",
    "female_have = pd.DataFrame(list(set(female1['paperid'])))\n",
    "female_have = pd.merge(female_have, lst, on=0, how='left')\n",
    "female_have.rename(columns = {0: 'paperid'}, inplace = True)\n",
    "\n",
    "## female_first\n",
    "female_first = df.loc[df['sequence_number']==1, :]\n",
    "female_first = female_first.drop_duplicates()\n",
    "female_first = female_first[['paperid', 'gender']]\n",
    "female_first.rename(columns = {'gender': 'female_first'}, inplace = True)\n",
    "\n",
    "## female_last\n",
    "female_last = df.loc[df['sequence_number']!=1, :]\n",
    "female_last = female_last.sort_values(by=['paperid', 'sequence_number'] ,ascending=True)\n",
    "female_last = female_last.drop_duplicates(['paperid'], keep='last')\n",
    "female_last = female_last[['paperid', 'gender']]\n",
    "female_last.rename(columns = {'gender': 'female_last'}, inplace = True)\n",
    "\n",
    "## number of authors\n",
    "author_n = df[['paperid', 'sequence_number']]\n",
    "author_n = author_n.sort_values(by=['paperid', 'sequence_number'] ,ascending=True)\n",
    "author_n = author_n.drop_duplicates(['paperid'], keep='last')\n",
    "\n",
    "author_n.rename(columns = {'sequence_number': 'author_n'}, inplace = True)\n",
    "\n",
    "\n",
    "female = pd.merge(female_have, female_first, on='paperid', how='left')\n",
    "female = pd.merge(female, female_last, on='paperid', how='left')\n",
    "female = pd.merge(female, author_n, on='paperid', how='left')\n",
    "\n",
    "female.to_csv(\"female.txt\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fields of papers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Making variable which is fields of papers ###\n",
    "\n",
    "## extracing data in \"PaperFieldsOfStudy.txt\" between 2016 and 2020\n",
    "paper_id = pd.read_table('paperid.txt', header=None) #load paperid between 2016 and 2020\n",
    "paper_id = paper_id.drop_duplicates()\n",
    "paper_id[\"over16\"] = 1\n",
    "paper_id.rename(columns = {0: 'id'}, inplace = True)\n",
    "\n",
    "#PapersFieldsOfStudy: paperid fieldofstudyid over16\n",
    "df = pd.read_table(\"PaperFieldsOfStudy.txt\", header=None, encoding = 'ISO-8859-1', quoting = 3) #MAG data\n",
    "df.rename(columns = {0: 'id'}, inplace = True)\n",
    "\n",
    "df = pd.merge(df, paper_id, left_on='id', right_on='id', how='left')\n",
    "df = df.loc[df['over16']==1, :]\n",
    "df.to_csv(\"paper_field.txt\", index=False, header=None, sep=\"\\t\", mode='a')\n",
    "\n",
    "##Using fields list of MAG\n",
    "#In PaperFieldsOfStudy.txt, each paper has multiple level of fields\n",
    "#We then use the most specific fields(level0)\n",
    "#Each paper has also multiple field in level0 and socre means corresponding degree in each area\n",
    "#We use a field when the score is the highest.\n",
    "journal = pd.read_table('FieldsOfStudy.txt', header=None) #MAG data\n",
    "journal_id = pd.DataFrame(journal.loc[journal[5]==0, :][0])\n",
    "journal_id = journal_id.reset_index(drop= True, inplace=False)\n",
    "journal_id.rename(columns = {0: 'field_id'}, inplace = True)\n",
    "journal_id['level0'] = 1\n",
    "\n",
    "df = pd.read_table(\"paper_field.txt\", header=None, encoding = 'ISO-8859-1', error_bad_lines=False)\n",
    "df.rename(columns = {1: 'field_id'}, inplace = True)\n",
    "df = df.loc[df[3].notnull(), :]\n",
    "    \n",
    "df['field_id'] = df['field_id'].astype(float)\n",
    "df['field_id'] = df['field_id'].astype('int64')\n",
    "    \n",
    "df = pd.merge(df, journal_id, left_on='field_id', right_on='field_id', how='left')\n",
    "df = df.loc[df['level0']==1, :]\n",
    "\n",
    "#paper_jour: paperid, fieldofstudyid, score\n",
    "df.to_csv(\"paper_jour.txt\", index=False, header=None, sep=\"\\t\") \n",
    "\n",
    "paper_jour= pd.read_table(\"paper_jour.txt\", sep=\"\\t\", header=None)\n",
    "paper_jour= paper_jour.sort_values(by=[0, 2] ,ascending=False)\n",
    "\n",
    "paper_jour2 = paper_jour.groupby(0)[2].rank(method=\"min\", ascending=False)\n",
    "paper_jour2 = pd.DataFrame(paper_jour2)\n",
    "paper_jour[\"rank\"] = paper_jour2[2]\n",
    "\n",
    "paper_jour.columns = ['paperid', 'field_id', 'score', 'over16', 'level0', 'rank']\n",
    "\n",
    "## field_n, number of fields\n",
    "field_n = paper_jour.drop_duplicates(['paperid'], keep='last')\n",
    "field_n = field_n[['paperid', 'rank']]\n",
    "field_n.rename(columns = {'rank':\"field_n\"}, inplace = True)\n",
    "\n",
    "## Making variable which the three high-scoring fields\n",
    "## field1, field2, field3\n",
    "paper_jour = paper_jour[['paperid', 'field_id', 'rank']]\n",
    "\n",
    "rank1 = paper_jour.loc[paper_jour[\"rank\"]==1, :]\n",
    "rank2 = paper_jour.loc[paper_jour[\"rank\"]==2, :]\n",
    "rank3 = paper_jour.loc[paper_jour[\"rank\"]==3, :]\n",
    "\n",
    "rank1.rename(columns = {'field_id': 'field1'}, inplace = True)\n",
    "rank2.rename(columns = {'field_id': 'field2'}, inplace = True)\n",
    "rank3.rename(columns = {'field_id': 'field3'}, inplace = True)\n",
    "\n",
    "rank = pd.merge(rank1, rank2, on='paperid', how='left')\n",
    "rank = pd.merge(rank, rank3, on='paperid', how='left')\n",
    "rank = rank[['paperid', 'field1', 'field2', 'field3']]\n",
    "\n",
    "#rank: paperid, field1, field2, field3\n",
    "rank = pd.merge(rank, field_n, on='paperid', how='left')\n",
    "\n",
    "## narrowing down fields from 19 to 5\n",
    "tag = pd.read_table('discipline.txt',sep='\\t', header=None) #we reclassify fields into 5 and make discipline.txt\n",
    "tag = tag.reset_index(drop=True).reset_index()\n",
    "tag[3] = tag.apply(lambda x : x['index']+11, axis = 1)\n",
    "tag.columns = ['index', 'field_id', 'field', 'id2', 'id1']\n",
    "\n",
    "dic = {'humanities':1, 'engineering':2, 'socialscience':3, 'nature':4, 'medicine':5}\n",
    "tag = tag.replace(dic)\n",
    "\n",
    "tag1 = dict(zip(tag.field_id, tag.id1))\n",
    "tag2 = dict(zip(tag.field_id, tag.id2))\n",
    "\n",
    "rank['field_id1'] = rank['field1']\n",
    "rank['field_id1'] = rank['field_id1'].replace(tag1)\n",
    "\n",
    "rank['field_id2'] = rank['field1']\n",
    "rank['field_id2'] = rank['field_id2'].replace(tag2)\n",
    "\n",
    "rank = rank[[\"paperid\", \"field_id1\", \"field_id2\", \"field_n\", \"field1\", \"field2\", \"field3\"]]\n",
    "rank.to_csv(\"rank.txt\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Countries of papers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Figure out countries of authors using affiliation's information ###\n",
    "\n",
    "paper_author = pd.read_table(\"paper_author.txt\", header=None)\n",
    "paper_author = paper_author.drop([3], axis=1)\n",
    "paper_author.columns = ['paperid', 'authorid', 'sequence_number']\n",
    "\n",
    "author_name = pd.read_table('author_name.txt', header=None)\n",
    "author_name = author_name.drop([1, 3], axis=1)\n",
    "author_name.columns = ['authorid', 'affiliation']\n",
    "\n",
    "paper_author = pd.merge(paper_author, author_name, on='authorid', how='left')\n",
    "\n",
    "## Affiliaitons.txt give a information of author's affiliation\n",
    "paper_aff = pd.read_table('Affiliations.txt', header=None) #MAG data\n",
    "paper_aff = paper_aff[[0, 10]]\n",
    "paper_aff.rename(columns = {0: 'affiliation', 10:'country'}, inplace = True)\n",
    "\n",
    "paper_author = pd.merge(paper_author, paper_aff, on='affiliation', how='left')\n",
    "\n",
    "paper_author.to_csv(\"pa_au_aff.txt\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### country by each author and by sequence number\n",
    "## country, country1, country2, country3\n",
    "\n",
    "pa_au_aff = pd.read_table(\"pa_au_aff.txt\")\n",
    "\n",
    "country = pd.DataFrame(list(set(pa_au_aff['paperid'])))\n",
    "country.rename(columns = {0: 'paperid'}, inplace = True)\n",
    "\n",
    "pa_au_aff = pa_au_aff.drop_duplicates()\n",
    "\n",
    "country1 = pa_au_aff.loc[pa_au_aff['sequence_number']==1, :]\n",
    "country1 = country1[['paperid', 'country']]\n",
    "\n",
    "country2 = pa_au_aff.loc[pa_au_aff['sequence_number']==2, :]\n",
    "country2 = country2[['paperid', 'country']]\n",
    "\n",
    "country3 = pa_au_aff.loc[pa_au_aff['sequence_number']==3, :]\n",
    "country3 = country3[['paperid', 'country']]\n",
    "\n",
    "country = pd.merge(country, country1, on='paperid', how='left')\n",
    "country = pd.merge(country, country2, on='paperid', how='left')\n",
    "country = pd.merge(country, country3, on='paperid', how='left')\n",
    "\n",
    "country.columns = ['paperid', 'country1', 'country2', 'country3']\n",
    "\n",
    "### number of countries ###\n",
    "##country_n\n",
    "country_n = pa_au_aff.loc[pa_au_aff['country'].notnull(), :]\n",
    "country_n = country_n[['paperid', 'country']]\n",
    "country_n = country_n.drop_duplicates()\n",
    "\n",
    "country_n = country_n.groupby(['paperid']).count().reset_index()\n",
    "country_n.rename(columns = {'country': 'country_n'}, inplace = True)\n",
    "\n",
    "country = pd.merge(country, country_n, on='paperid', how='left')\n",
    "\n",
    "country.to_csv(\"country.txt\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Making individual/national characteristics ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### individual characteristics\n",
    "\n",
    "### h-index by author and affiliation \n",
    "pa_au_aff = pd.read_table(\"pa_au_aff.txt\")\n",
    "pa_au_aff = pa_au_aff[['paperid', 'authorid', 'affiliation']]\n",
    "\n",
    "aff = pd.read_table(\"MAG_210215_AffiliationInfo10Y.tsv\", sep=\"\\t\") #MAG metadata\n",
    "aff = aff[['AffiliationId', 'HIdx(JourConfRepo)']]\n",
    "aff.rename(columns = {'HIdx(JourConfRepo)': 'aff_hidx', 'AffiliationId':'affiliation'}, inplace = True)\n",
    "\n",
    "aut = pd.read_table(\"MAG_210215_AuthorInfo10Y.tsv\", sep=\"\\t\")\n",
    "aut = aut[['AuthorId', 'HIdx(JourConfRepo)']]\n",
    "aut.rename(columns = {'HIdx(JourConfRepo)': 'aut_hidx', 'AuthorId':'authorid'}, inplace = True)\n",
    "\n",
    "pa_au_aff = pd.merge(pa_au_aff, aff, on='affiliation', how='left')\n",
    "pa_au_aff = pd.merge(pa_au_aff, aut, on='authorid', how='left')\n",
    "\n",
    "pa_au_aff = pa_au_aff.loc[pa_au_aff['authorid'].notnull(), :]\n",
    "\n",
    "### academic_age\n",
    "aca_age = pd.read_table(\"MAG_210215_AuthorInfo.tsv\") #MAG metadata\n",
    "\n",
    "aca_age = aca_age[['AuthorId', 'FirstPub(DateJourConfRepo)']]\n",
    "aca_age = aca_age.loc[aca_age['FirstPub(DateJourConfRepo)'].notnull(), :]\n",
    "\n",
    "#deleting outlier\n",
    "aca_age = aca_age.loc[(aca_age['FirstPub(DateJourConfRepo)']>=1960)&(aca_age['FirstPub(DateJourConfRepo)']<=2020), :]\n",
    "\n",
    "#calculating academic age from present to time of a first paper\n",
    "aca_age['aca_age']= aca_age.apply(lambda x : 2020 - x['FirstPub(DateJourConfRepo)'], axis = 1)\n",
    "\n",
    "aca_age = aca_age.drop(['FirstPub(DateJourConfRepo)'], axis=1) #AuthorId, aca_age\n",
    "aca_age.rename(columns = {'AuthorId': 'authorid'}, inplace = True)\n",
    "\n",
    "aca_age.to_csv(\"aca_age1.txt\", index=False, sep='\\t')\n",
    "\n",
    "aca_age = pd.read_table(\"aca_age1.txt\")\n",
    "pa_au_aff = pd.merge(pa_au_aff, aca_age, on='authorid', how='left')\n",
    "pa_au_aff.to_csv(\"pa_au_aff2.txt\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (3,4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "### national characteristics ###\n",
    "\n",
    "## we merge between mag and data from othersources by country \n",
    "\n",
    "##using iso_code in MAG data\n",
    "paper_aff = pd.read_table('Affiliations.txt', header=None) #MAG data\n",
    "paper_aff = paper_aff[[0, 10]]\n",
    "paper_aff.rename(columns = {0: 'AffiliationId', 10:'iso_code_3'}, inplace = True)\n",
    "paper_aff = paper_aff.loc[paper_aff['iso_code_3'].notnull(), :]\n",
    "\n",
    "#total cases per million/total deaths per million\n",
    "covid = pd.read_csv('covid.csv') #https://github.com/owid/covid-19-data/tree/master/public/data\n",
    "covid['date'] = pd.to_datetime(covid['date'])\n",
    "covid = covid.loc[covid['date']<\"2021\"] #last date in 2020\n",
    "covid = covid.sort_values(by=['iso_code', 'date'] ,ascending=True)\n",
    "covid = covid.drop_duplicates(['iso_code'], keep='last')\n",
    "\n",
    "covid = covid.loc[covid['total_cases_per_million'].notnull(), :]\n",
    "covid = covid.loc[covid['total_deaths_per_million'].notnull(), :]\n",
    "\n",
    "covid = covid[['iso_code', 'iso_code_3', 'total_cases_per_million', 'total_deaths_per_million',]]\n",
    "covid = covid.loc[covid['iso_code_3'].notnull(), :]\n",
    "\n",
    "#Workplace/Residential\n",
    "gm = pd.read_csv(\"Global_Mobility_Report.csv\", encoding='utf-8') #https://www.google.com/covid19/mobility\n",
    "\n",
    "gm['date'] = pd.to_datetime(gm['date'])\n",
    "gm = gm.loc[gm['date']<\"2021\"] #2020년 마지막 보고 기준\n",
    "gm = gm.groupby(['country_region_code']).mean().reset_index()\n",
    "\n",
    "gm.rename(columns = {'workplaces_percent_change_from_baseline': 'Workplace', \n",
    "                     'residential_percent_change_from_baseline':'Residential', \n",
    "                     'country_region_code': 'iso_code_3'}, inplace = True)\n",
    "gm = pd.merge(gm, covid, on='iso_code_3', how='left')\n",
    "\n",
    "gm = gm[['iso_code_3', 'Workplace', 'Residential']]\n",
    "\n",
    "#gender equality\n",
    "ggis = pd.read_csv('data_indicators_country.csv', encoding='cp949') #https://github.com/fverkroost/epjds-professional-gender-gaps\n",
    "ggis = ggis[['iso3code', 'sex_ratio_linkedin_overall_filtered']]\n",
    "#filter: The observation filter identifies those countries for which audience counts in that category of at least 1000 were available.\n",
    "\n",
    "ggis.rename(columns = {'iso3code':'iso_code', 'sex_ratio_linkedin_overall_filtered': 'ggis1', 'sex_ratio_ilostat_prof_tech_filtered':'ggis2' }, inplace = True)\n",
    "\n",
    "ggis1 = ggis.loc[ggis['ggis1'].notnull(), :][['iso_code', 'ggis1']]\n",
    "\n",
    "#merging\n",
    "paper_aff = pd.merge(paper_aff, covid, on='iso_code_3', how='left')\n",
    "paper_aff = pd.merge(paper_aff, gm, on='iso_code_3', how='left')\n",
    "paper_aff = pd.merge(paper_aff, ggis1, on='iso_code', how='left')\n",
    "\n",
    "paper_aff.rename(columns = {'AffiliationId': 'affiliation'}, inplace = True)\n",
    "\n",
    "paper_aff.to_csv(\"paper_aff.txt\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#merging individual and natioanl data\n",
    "pa_au_aff = pd.read_table(\"pa_au_aff2.txt\")\n",
    "\n",
    "pa_au_aff = pd.merge(pa_au_aff, paper_aff, on='affiliation', how='left')\n",
    "\n",
    "pa_au_aff = pa_au_aff.groupby(['paperid']).mean().reset_index()\n",
    "pa_au_aff = pa_au_aff.drop(['authorid', 'affiliation'], axis=1)\n",
    " \n",
    "pa_au_aff.to_csv(\"other_var.txt\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Merging all variables and making dataset by paperid ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### merging all variables and split online/offline data ###\n",
    "\n",
    "date = pd.read_table('papers_2016_sub_total2.txt')\n",
    "\n",
    "female = pd.read_table('female.txt')\n",
    "\n",
    "rank = pd.read_table('rank.txt')\n",
    "\n",
    "country = pd.read_table('country.txt')\n",
    "\n",
    "other_var = pd.read_table('other_var.txt')\n",
    "\n",
    "date = pd.merge(date, female, on='paperid', how='left')\n",
    "date = pd.merge(date, rank, on='paperid', how='left')\n",
    "date = pd.merge(date, country, on='paperid', how='left')\n",
    "date = pd.merge(date, other_var, on='paperid', how='left')\n",
    "\n",
    "#paper type\n",
    "dic =  {\"Repository\": 1, \"Journal\": 2, \"Thesis\": 3, \"Conference\": 4, \"Patent\": 5,  \"Book\": 6, \"BookChapter\": 7, \"Dataset\": 8}\n",
    "\n",
    "#saving offline data\n",
    "offline = date.loc[date['onoff']==1, :]\n",
    "offline['paper_type'] = offline['paper_type'].replace(dic)\n",
    "offline = offline.loc[(offline['paper_type']<=2)|(offline['paper_type']==4), :]\n",
    "offline.to_csv(\"offline_0627.txt\", index=False, sep='\\t')\n",
    "\n",
    "#saving online data\n",
    "online = date.loc[date['onoff']==2, :]\n",
    "online['paper_type'] = online['paper_type'].replace(dic)\n",
    "online = online.loc[(online['paper_type']<=2)|(online['paper_type']==4), :]\n",
    "online.to_csv(\"online_0627.txt\",index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Making dataset by authorid ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### making individual/national characteristics dataset by authorid ###\n",
    "pa_au_aff = pd.read_table(\"pa_au_aff2.txt\")\n",
    "\n",
    "pa_au_aff = pd.merge(pa_au_aff, paper_aff, on='affiliation', how='left')\n",
    "\n",
    "pa_au_aff = pa_au_aff.drop(['paperid'], axis=1)\n",
    "pa_au_aff = pa_au_aff.drop_duplicates()\n",
    "\n",
    "pa_au_aff = pa_au_aff.loc[pa_au_aff['affiliation'].notnull(), :]\n",
    "\n",
    "pa_au_aff.to_csv(\"effect_var.txt\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### aca_age_sub ###\n",
    "pa_au_aff = pd.read_table(\"pa_au_aff.txt\")\n",
    "pa_au_aff = pa_au_aff[['paperid', 'authorid', 'sequence_number']]\n",
    "\n",
    "aca_age = pd.read_table(\"aca_age1.txt\")\n",
    "pa_au_aff = pd.merge(pa_au_aff, aca_age, on='authorid', how='left')\n",
    "\n",
    "pa_au_aff.to_csv(\"aca_age2.txt\", index=False, sep='\\t') #paperid, authorid, sequence_number, aca_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### making dataset by authorid - offline ###\n",
    "\n",
    "##loading dataset(by paperid)\n",
    "offline = pd.read_table('offline_0627.txt')\n",
    "\n",
    "offline = offline.loc[offline['author_n']<= 50, :] #under 50\n",
    "offline = offline.loc[offline['paper_type']==2, :] #only use journal\n",
    "offline = offline.loc[offline['field_id1'].notnull(), :]\n",
    "\n",
    "offline = offline[['paperid', 'paper_type', 'year1', 'month1', 'field_id1']]\n",
    "\n",
    "aca_age2 = pd.read_table('aca_age2.txt')\n",
    "aca_age2 = aca_age2.drop_duplicates()\n",
    "\n",
    "author_gender = pd.read_table(\"author_gender.txt\")\n",
    "\n",
    "aca_age2 = pd.merge(aca_age2, offline, on='paperid', how='left')\n",
    "aca_age2 = aca_age2.loc[aca_age2['paper_type'].notnull(), :]\n",
    "\n",
    "aca_age2 = pd.merge(aca_age2, author_gender, on='authorid', how='left')\n",
    "\n",
    "aca_age2[\"age_year\"] = aca_age2.apply(lambda x : x['aca_age'] - (2020 - x['year1']), axis = 1)\n",
    "\n",
    "aca_age2 = aca_age2.sort_values(by=['paperid', 'sequence_number'] ,ascending=True)\n",
    "aca_age2[\"id\"] = aca_age2.apply(lambda x : str(x['paperid']) +','+ str(x['sequence_number']), axis = 1)\n",
    "\n",
    "female_last = aca_age2.drop_duplicates(['paperid'], keep='last')\n",
    "female_last[\"author_role\"] = 2\n",
    "female_last = female_last.loc[female_last['sequence_number']!=1, :]\n",
    "female_last = female_last[['id', 'author_role']]\n",
    "\n",
    "aca_age2 = pd.merge(aca_age2, female_last, on='id', how='left')\n",
    "\n",
    "aca_age2.loc[aca_age2['sequence_number']==1, \"author_role\"] =\"제1\"\n",
    "aca_age2.loc[aca_age2['author_role']==2, \"author_role\"] =\"교신\"\n",
    "aca_age2.loc[aca_age2['author_role'].isnull(), \"author_role\"] =\"기타\"\n",
    "\n",
    "aca_age2 = aca_age2.drop(['id', 'paper_type', \"aca_age\"], axis =1)\n",
    "\n",
    "aca_age2.to_csv(\"author_role_plot_0627.txt\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### making dataset by authorid - online ###\n",
    "\n",
    "##loading dataset(by paperid)\n",
    "online = pd.read_table('online_0627.txt')\n",
    "\n",
    "online = online.loc[online['author_n']<= 50, :]\n",
    "online = online.loc[online['paper_type']==1, :] #only use repository\n",
    "online = online.loc[online['field_id1'].notnull(), :]\n",
    "\n",
    "online = online[['paperid', 'paper_type', 'year2', 'month2', 'field_id1']]\n",
    "\n",
    "aca_age2 = pd.read_table('aca_age2.txt')\n",
    "aca_age2 = aca_age2.drop_duplicates()\n",
    "\n",
    "author_gender = pd.read_table(\"author_gender.txt\")\n",
    "\n",
    "aca_age2 = pd.merge(aca_age2, online, on='paperid', how='left')\n",
    "aca_age2 = aca_age2.loc[aca_age2['paper_type'].notnull(), :]\n",
    "\n",
    "aca_age2 = pd.merge(aca_age2, author_gender, on='authorid', how='left')\n",
    "\n",
    "aca_age2[\"age_year\"] = aca_age2.apply(lambda x : x['aca_age'] - (2020 - x['year1']), axis = 1)\n",
    "\n",
    "aca_age2 = aca_age2.sort_values(by=['paperid', 'sequence_number'] ,ascending=True)\n",
    "aca_age2[\"id\"] = aca_age2.apply(lambda x : str(x['paperid']) +','+ str(x['sequence_number']), axis = 1)\n",
    "\n",
    "female_last = aca_age2.drop_duplicates(['paperid'], keep='last')\n",
    "female_last[\"author_role\"] = 2\n",
    "female_last = female_last.loc[female_last['sequence_number']!=1, :]\n",
    "female_last = female_last[['id', 'author_role']]\n",
    "\n",
    "aca_age2 = pd.merge(aca_age2, female_last, on='id', how='left')\n",
    "\n",
    "aca_age2.loc[aca_age2['sequence_number']==1, \"author_role\"] =\"제1\"\n",
    "aca_age2.loc[aca_age2['author_role']==2, \"author_role\"] =\"교신\"\n",
    "aca_age2.loc[aca_age2['author_role'].isnull(), \"author_role\"] =\"기타\"\n",
    "\n",
    "aca_age2 = aca_age2.drop(['id', 'paper_type', \"aca_age\"], axis =1)\n",
    "\n",
    "aca_age2.to_csv(\"author_role_plot_on_0627.txt\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Making dataset for XGBoost ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### offline\n",
    "offline = pd.read_table('offline_0627.txt')\n",
    "\n",
    "offline = offline.loc[offline['author_n']<= 50, :]\n",
    "offline = offline.loc[offline['paper_type']==2, :]\n",
    "offline = offline.loc[offline['field_id1'].notnull(), :]\n",
    "\n",
    "offline.rename(columns = {'year1': 'year'}, inplace = True)\n",
    "\n",
    "off = offline[['paperid', 'year', 'female_have', 'female_first', 'female_last']]\n",
    "\n",
    "author_role = pd.read_table('author_role_plot_0627.txt')\n",
    "\n",
    "mean= author_role[['paperid', 'authorid']]\n",
    "mean = pd.merge(mean, df_var, on='authorid', how='left')\n",
    "mean = mean[['paperid', 'aca_age', 'aut_hidx', 'aff_hidx', 'ggis1', 'total_cases_per_million', 'total_deaths_per_million', 'Workplace', 'Residential']]\n",
    "mean = mean.groupby(['paperid']).mean().reset_index()\n",
    "mean = mean.drop_duplicates()\n",
    "\n",
    "first = author_role.loc[author_role['author_role'] == '제1', :]\n",
    "first = first[['paperid', 'authorid']]\n",
    "first = pd.merge(first, df_var, on='authorid', how='left')\n",
    "first = first[['paperid', 'aca_age', 'aut_hidx', 'aff_hidx', 'ggis1', 'total_cases_per_million', 'total_deaths_per_million', 'Workplace', 'Residential']]\n",
    "first = first.drop_duplicates()\n",
    "first.rename(columns = {'aca_age':'aca_age_f', 'aut_hidx':'aut_hidx_f', 'aff_hidx':'aff_hidx_f', 'ggis1':'ggis1_f', \n",
    "                        'total_cases_per_million':'cases_f', 'total_deaths_per_million':'deaths_f', 'Workplace':'work_f', 'Residential':'res_f'}, inplace = True)\n",
    "\n",
    "last = author_role.loc[author_role['author_role'] == '교신', :]\n",
    "last = last[['paperid', 'authorid']]\n",
    "last = pd.merge(last, df_var, on='authorid', how='left')\n",
    "last = last[['paperid', 'aca_age', 'aut_hidx', 'aff_hidx', 'ggis1', 'total_cases_per_million', 'total_deaths_per_million', 'Workplace', 'Residential']]\n",
    "last = last.drop_duplicates()\n",
    "last.rename(columns = {'aca_age':'aca_age_l', 'aut_hidx':'aut_hidx_l', 'aff_hidx':'aff_hidx_l', 'ggis1':'ggis1_l',\n",
    "                       'total_cases_per_million':'cases_l', 'total_deaths_per_million':'deaths_l', 'Workplace':'work_l', 'Residential':'res_l'}, inplace = True)\n",
    "\n",
    "logis = pd.merge(mean, first, on='paperid', how='left')\n",
    "logis = pd.merge(logis, last, on='paperid', how='left')\n",
    "logis = pd.merge(off, logis, on='paperid', how='left')\n",
    "\n",
    "logis.to_csv(\"logistic_0828.txt\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### offline\n",
    "online = pd.read_table('online_0627.txt')\n",
    "\n",
    "online = online.loc[online['author_n']<= 50, :]\n",
    "online = online.loc[online['paper_type']==1, :]\n",
    "online = online.loc[online['field_id1'].notnull(), :]\n",
    "\n",
    "online.rename(columns = {'year2': 'year'}, inplace = True)\n",
    "\n",
    "on = online[['paperid', 'year', 'female_have', 'female_first', 'female_last']]\n",
    "\n",
    "author_role = pd.read_table('author_role_plot_on_0627.txt')\n",
    "\n",
    "mean= author_role[['paperid', 'authorid']]\n",
    "mean = pd.merge(mean, df_var, on='authorid', how='left')\n",
    "mean = mean[['paperid', 'aca_age', 'aut_hidx', 'aff_hidx', 'ggis1', 'total_cases_per_million', 'total_deaths_per_million', 'Workplace', 'Residential']]\n",
    "mean = mean.groupby(['paperid']).mean().reset_index()\n",
    "mean = mean.drop_duplicates()\n",
    "\n",
    "first = author_role.loc[author_role['author_role'] == '제1', :]\n",
    "first = first[['paperid', 'authorid']]\n",
    "first = pd.merge(first, df_var, on='authorid', how='left')\n",
    "first = first[['paperid', 'aca_age', 'aut_hidx', 'aff_hidx', 'ggis1', 'total_cases_per_million', 'total_deaths_per_million', 'Workplace', 'Residential']]\n",
    "first = first.drop_duplicates()\n",
    "first.rename(columns = {'aca_age':'aca_age_f', 'aut_hidx':'aut_hidx_f', 'aff_hidx':'aff_hidx_f', 'ggis1':'ggis1_f', \n",
    "                        'total_cases_per_million':'cases_f', 'total_deaths_per_million':'deaths_f', 'Workplace':'work_f', 'Residential':'res_f'}, inplace = True)\n",
    "\n",
    "last = author_role.loc[author_role['author_role'] == '교신', :]\n",
    "last = last[['paperid', 'authorid']]\n",
    "last = pd.merge(last, df_var, on='authorid', how='left')\n",
    "last = last[['paperid', 'aca_age', 'aut_hidx', 'aff_hidx', 'ggis1', 'total_cases_per_million', 'total_deaths_per_million', 'Workplace', 'Residential']]\n",
    "last = last.drop_duplicates()\n",
    "last.rename(columns = {'aca_age':'aca_age_l', 'aut_hidx':'aut_hidx_l', 'aff_hidx':'aff_hidx_l', 'ggis1':'ggis1_l',\n",
    "                       'total_cases_per_million':'cases_l', 'total_deaths_per_million':'deaths_l', 'Workplace':'work_l', 'Residential':'res_l'}, inplace = True)\n",
    "\n",
    "logis = pd.merge(mean, first, on='paperid', how='left')\n",
    "logis = pd.merge(logis, last, on='paperid', how='left')\n",
    "logis = pd.merge(on, logis, on='paperid', how='left')\n",
    "\n",
    "logis.to_csv(\"logistic_0828_on.txt\", sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
